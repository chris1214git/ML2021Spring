{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYlaRwNu7ojq"
   },
   "source": [
    "# **Homework 2-1 Phoneme Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emUd7uS7crTz"
   },
   "source": [
    "## The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT)\n",
    "The TIMIT corpus of reading speech has been designed to provide speech data for the acquisition of acoustic-phonetic knowledge and for the development and evaluation of automatic speech recognition systems.\n",
    "\n",
    "This homework is a multiclass classification task, \n",
    "we are going to train a deep neural network classifier to predict the phonemes for each frame from the speech corpus TIMIT.\n",
    "\n",
    "link: https://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVUGfWTo7_Oj"
   },
   "source": [
    "## Download Data\n",
    "Download data from google drive, then unzip it.\n",
    "\n",
    "You should have `timit_11/train_11.npy`, `timit_11/train_label_11.npy`, and `timit_11/test_11.npy` after running this block.<br><br>\n",
    "`timit_11/`\n",
    "- `train_11.npy`: training data<br>\n",
    "- `train_label_11.npy`: training label<br>\n",
    "- `test_11.npy`:  testing data<br><br>\n",
    "\n",
    "**notes: if the google drive link is dead, you can download the data directly from Kaggle and upload it to the workspace**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L_4anls8Drv"
   },
   "source": [
    "## Preparing Data\n",
    "Load the training and testing data from the `.npy` file (NumPy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJjLT8em-y9G",
    "outputId": "c15b327f-a646-4d38-f89d-e3a5f44ceed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Size of training data: (1229932, 429)\n",
      "Size of testing data: (451552, 429)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "data_root='./timit_11/'\n",
    "train = np.load(data_root + 'train_11.npy')\n",
    "train_label = np.load(data_root + 'train_label_11.npy')\n",
    "test = np.load(data_root + 'test_11.npy')\n",
    "\n",
    "print('Size of training data: {}'.format(train.shape))\n",
    "print('Size of testing data: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "us5XW_x6udZQ"
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fjf5EcmJtf4e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TIMITDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(np.int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otIC6WhGeh9v"
   },
   "source": [
    "Split the labeled data into a training set and a validation set, you can modify the variable `VAL_RATIO` to change the ratio of validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYqi_lAuvC59",
    "outputId": "f34c3ee6-5966-43f4-f50a-326f215eee40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: (983945, 429)\n",
      "Size of validation set: (245987, 429)\n"
     ]
    }
   ],
   "source": [
    "VAL_RATIO = 0.2\n",
    "\n",
    "percent = int(train.shape[0] * (1 - VAL_RATIO))\n",
    "train_x, train_y, val_x, val_y = train[:percent], train_label[:percent], train[percent:], train_label[percent:]\n",
    "print('Size of training set: {}'.format(train_x.shape))\n",
    "print('Size of validation set: {}'.format(val_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbCfclUIgMTX"
   },
   "source": [
    "Create a data loader from the dataset, feel free to tweak the variable `BATCH_SIZE` here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SY7X0lUgb50"
   },
   "source": [
    "Cleanup the unneeded variables to save memory.<br>\n",
    "\n",
    "**notes: if you need to use these variables later, then you may remove this block or clean up unneeded variables later<br>the data size is quite huge, so be aware of memory usage in colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRqKNvNZwe3V"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m1e359DRaEwh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import autokeras as ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQHMTUEcZ6xM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "structured_data...|True              |?                 \n",
      "structured_data...|2                 |?                 \n",
      "structured_data...|False             |?                 \n",
      "structured_data...|0                 |?                 \n",
      "structured_data...|32                |?                 \n",
      "structured_data...|32                |?                 \n",
      "classification_...|0                 |?                 \n",
      "optimizer         |adam              |?                 \n",
      "learning_rate     |0.001             |?                 \n",
      "\n",
      "Epoch 1/20\n",
      "30749/30749 [==============================] - 257s 8ms/step - loss: 1.6984 - accuracy: 0.4955 - val_loss: 1.5481 - val_accuracy: 0.5311\n",
      "Epoch 2/20\n",
      "30749/30749 [==============================] - 257s 8ms/step - loss: 1.3982 - accuracy: 0.5707 - val_loss: 1.5062 - val_accuracy: 0.5437\n",
      "Epoch 3/20\n",
      "30749/30749 [==============================] - 257s 8ms/step - loss: 1.3632 - accuracy: 0.5791 - val_loss: 1.4801 - val_accuracy: 0.5498\n",
      "Epoch 4/20\n",
      "30749/30749 [==============================] - 254s 8ms/step - loss: 1.3473 - accuracy: 0.5847 - val_loss: 1.4679 - val_accuracy: 0.5537\n",
      "Epoch 5/20\n",
      "30749/30749 [==============================] - 255s 8ms/step - loss: 1.3379 - accuracy: 0.5874 - val_loss: 1.4565 - val_accuracy: 0.5571\n",
      "Epoch 6/20\n",
      "30749/30749 [==============================] - 254s 8ms/step - loss: 1.3305 - accuracy: 0.5901 - val_loss: 1.4564 - val_accuracy: 0.5582\n",
      "Epoch 7/20\n",
      "30749/30749 [==============================] - 254s 8ms/step - loss: 1.3259 - accuracy: 0.5915 - val_loss: 1.4546 - val_accuracy: 0.5582\n",
      "Epoch 8/20\n",
      "30749/30749 [==============================] - 254s 8ms/step - loss: 1.3223 - accuracy: 0.5926 - val_loss: 1.4581 - val_accuracy: 0.5570\n",
      "Epoch 9/20\n",
      "30749/30749 [==============================] - 253s 8ms/step - loss: 1.3190 - accuracy: 0.5935 - val_loss: 1.4617 - val_accuracy: 0.5564\n",
      "Epoch 10/20\n",
      "30749/30749 [==============================] - 260s 8ms/step - loss: 1.3165 - accuracy: 0.5937 - val_loss: 1.4559 - val_accuracy: 0.5575\n",
      "Epoch 11/20\n",
      "30749/30749 [==============================] - 278s 9ms/step - loss: 1.3143 - accuracy: 0.5938 - val_loss: 1.4496 - val_accuracy: 0.5595\n",
      "Epoch 12/20\n",
      "17484/30749 [================>.............] - ETA: 1:33 - loss: 1.3152 - accuracy: 0.5944"
     ]
    }
   ],
   "source": [
    "# Initialize the structured data classifier.\n",
    "clf = ak.StructuredDataClassifier(\n",
    "    overwrite=True,\n",
    "    max_trials=3,\n",
    "    tuner=None,\n",
    "    seed=33\n",
    ")  \n",
    "# Feed the structured data classifier with training data.\n",
    "clf.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=20, \n",
    "    batch_size=320,\n",
    "    validation_data=(val_x, val_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3yrMhQPfR4A"
   },
   "outputs": [],
   "source": [
    "# Predict with the best model.\n",
    "train_pred = clf.predict(train_x)\n",
    "val_pred = clf.predict(val_x)\n",
    "test_pred = clf.predict(test)\n",
    "\n",
    "print('Train accuracy', (train_pred==train_y).sum() / train_y.shape[0])\n",
    "print('Valid accuracy', (val_pred==val_y).sum() / val_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Hi7jTn3PX-m"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfUECMFCn5VG"
   },
   "source": [
    "Create a testing dataset, and load model from the saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PKjtAScPWtr"
   },
   "outputs": [],
   "source": [
    "# # create testing dataset\n",
    "# test_set = TIMITDataset(test, None)\n",
    "# test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "940TtCCdoYd0"
   },
   "source": [
    "Make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9wlqdcCZ4Q1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWDf_C-omElb"
   },
   "source": [
    "Write prediction to a CSV file.\n",
    "\n",
    "After finish running this block, download the file `prediction.csv` from the files section on the left-hand side and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuljYSPHcZir"
   },
   "outputs": [],
   "source": [
    "with open('prediction.csv', 'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i, y in enumerate(predict):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SHARE MLSpring2021 - HW2-autokeras.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

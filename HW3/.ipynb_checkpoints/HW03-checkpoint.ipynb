{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_a2USyd4giE"
   },
   "source": [
    "# **Homework 3 - Convolutional Neural Network**\n",
    "\n",
    "This is the example code of homework 3 of the machine learning course by Prof. Hung-yi Lee.\n",
    "\n",
    "In this homework, you are required to build a convolutional neural network for image classification, possibly with some advanced training tips.\n",
    "\n",
    "\n",
    "There are three levels here:\n",
    "\n",
    "**Easy**: Build a simple convolutional neural network as the baseline. (2 pts)\n",
    "\n",
    "**Medium**: Design a better architecture or adopt different data augmentations to improve the performance. (2 pts)\n",
    "\n",
    "**Hard**: Utilize provided unlabeled data to obtain better results. (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHpJocsDr6iA"
   },
   "source": [
    "## **About the Dataset**\n",
    "\n",
    "The dataset used here is food-11, a collection of food images in 11 classes.\n",
    "\n",
    "For the requirement in the homework, TAs slightly modified the data.\n",
    "Please DO NOT access the original fully-labeled training data or testing labels.\n",
    "\n",
    "Also, the modified dataset is for this course only, and any further distribution or commercial use is forbidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adv_inception_v3',\n",
       " 'cspdarknet53',\n",
       " 'cspdarknet53_iabn',\n",
       " 'cspresnet50',\n",
       " 'cspresnet50d',\n",
       " 'cspresnet50w',\n",
       " 'cspresnext50',\n",
       " 'cspresnext50_iabn',\n",
       " 'darknet53',\n",
       " 'densenet121',\n",
       " 'densenet121d',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'densenet264',\n",
       " 'densenet264d_iabn',\n",
       " 'densenetblur121d',\n",
       " 'dla34',\n",
       " 'dla46_c',\n",
       " 'dla46x_c',\n",
       " 'dla60',\n",
       " 'dla60_res2net',\n",
       " 'dla60_res2next',\n",
       " 'dla60x',\n",
       " 'dla60x_c',\n",
       " 'dla102',\n",
       " 'dla102x',\n",
       " 'dla102x2',\n",
       " 'dla169',\n",
       " 'dm_nfnet_f0',\n",
       " 'dm_nfnet_f1',\n",
       " 'dm_nfnet_f2',\n",
       " 'dm_nfnet_f3',\n",
       " 'dm_nfnet_f4',\n",
       " 'dm_nfnet_f5',\n",
       " 'dm_nfnet_f6',\n",
       " 'dpn68',\n",
       " 'dpn68b',\n",
       " 'dpn92',\n",
       " 'dpn98',\n",
       " 'dpn107',\n",
       " 'dpn131',\n",
       " 'eca_vovnet39b',\n",
       " 'ecaresnet26t',\n",
       " 'ecaresnet50d',\n",
       " 'ecaresnet50d_pruned',\n",
       " 'ecaresnet50t',\n",
       " 'ecaresnet101d',\n",
       " 'ecaresnet101d_pruned',\n",
       " 'ecaresnet200d',\n",
       " 'ecaresnet269d',\n",
       " 'ecaresnetlight',\n",
       " 'ecaresnext26t_32x4d',\n",
       " 'ecaresnext50t_32x4d',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b1_pruned',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b2_pruned',\n",
       " 'efficientnet_b2a',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b3_pruned',\n",
       " 'efficientnet_b3a',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_b8',\n",
       " 'efficientnet_cc_b0_4e',\n",
       " 'efficientnet_cc_b0_8e',\n",
       " 'efficientnet_cc_b1_8e',\n",
       " 'efficientnet_el',\n",
       " 'efficientnet_em',\n",
       " 'efficientnet_es',\n",
       " 'efficientnet_l2',\n",
       " 'efficientnet_lite0',\n",
       " 'efficientnet_lite1',\n",
       " 'efficientnet_lite2',\n",
       " 'efficientnet_lite3',\n",
       " 'efficientnet_lite4',\n",
       " 'ens_adv_inception_resnet_v2',\n",
       " 'ese_vovnet19b_dw',\n",
       " 'ese_vovnet19b_slim',\n",
       " 'ese_vovnet19b_slim_dw',\n",
       " 'ese_vovnet39b',\n",
       " 'ese_vovnet39b_evos',\n",
       " 'ese_vovnet57b',\n",
       " 'ese_vovnet99b',\n",
       " 'ese_vovnet99b_iabn',\n",
       " 'fbnetc_100',\n",
       " 'gernet_l',\n",
       " 'gernet_m',\n",
       " 'gernet_s',\n",
       " 'gluon_inception_v3',\n",
       " 'gluon_resnet18_v1b',\n",
       " 'gluon_resnet34_v1b',\n",
       " 'gluon_resnet50_v1b',\n",
       " 'gluon_resnet50_v1c',\n",
       " 'gluon_resnet50_v1d',\n",
       " 'gluon_resnet50_v1s',\n",
       " 'gluon_resnet101_v1b',\n",
       " 'gluon_resnet101_v1c',\n",
       " 'gluon_resnet101_v1d',\n",
       " 'gluon_resnet101_v1s',\n",
       " 'gluon_resnet152_v1b',\n",
       " 'gluon_resnet152_v1c',\n",
       " 'gluon_resnet152_v1d',\n",
       " 'gluon_resnet152_v1s',\n",
       " 'gluon_resnext50_32x4d',\n",
       " 'gluon_resnext101_32x4d',\n",
       " 'gluon_resnext101_64x4d',\n",
       " 'gluon_senet154',\n",
       " 'gluon_seresnext50_32x4d',\n",
       " 'gluon_seresnext101_32x4d',\n",
       " 'gluon_seresnext101_64x4d',\n",
       " 'gluon_xception65',\n",
       " 'hrnet_w18',\n",
       " 'hrnet_w18_small',\n",
       " 'hrnet_w18_small_v2',\n",
       " 'hrnet_w30',\n",
       " 'hrnet_w32',\n",
       " 'hrnet_w40',\n",
       " 'hrnet_w44',\n",
       " 'hrnet_w48',\n",
       " 'hrnet_w64',\n",
       " 'ig_resnext101_32x8d',\n",
       " 'ig_resnext101_32x16d',\n",
       " 'ig_resnext101_32x32d',\n",
       " 'ig_resnext101_32x48d',\n",
       " 'inception_resnet_v2',\n",
       " 'inception_v3',\n",
       " 'inception_v4',\n",
       " 'legacy_senet154',\n",
       " 'legacy_seresnet18',\n",
       " 'legacy_seresnet34',\n",
       " 'legacy_seresnet50',\n",
       " 'legacy_seresnet101',\n",
       " 'legacy_seresnet152',\n",
       " 'legacy_seresnext26_32x4d',\n",
       " 'legacy_seresnext50_32x4d',\n",
       " 'legacy_seresnext101_32x4d',\n",
       " 'mixnet_l',\n",
       " 'mixnet_m',\n",
       " 'mixnet_s',\n",
       " 'mixnet_xl',\n",
       " 'mixnet_xxl',\n",
       " 'mnasnet_050',\n",
       " 'mnasnet_075',\n",
       " 'mnasnet_100',\n",
       " 'mnasnet_140',\n",
       " 'mnasnet_a1',\n",
       " 'mnasnet_b1',\n",
       " 'mnasnet_small',\n",
       " 'mobilenetv2_100',\n",
       " 'mobilenetv2_110d',\n",
       " 'mobilenetv2_120d',\n",
       " 'mobilenetv2_140',\n",
       " 'mobilenetv3_large_075',\n",
       " 'mobilenetv3_large_100',\n",
       " 'mobilenetv3_rw',\n",
       " 'mobilenetv3_small_075',\n",
       " 'mobilenetv3_small_100',\n",
       " 'nasnetalarge',\n",
       " 'nf_ecaresnet26',\n",
       " 'nf_ecaresnet50',\n",
       " 'nf_ecaresnet101',\n",
       " 'nf_regnet_b0',\n",
       " 'nf_regnet_b1',\n",
       " 'nf_regnet_b2',\n",
       " 'nf_regnet_b3',\n",
       " 'nf_regnet_b4',\n",
       " 'nf_regnet_b5',\n",
       " 'nf_resnet26',\n",
       " 'nf_resnet50',\n",
       " 'nf_resnet101',\n",
       " 'nf_seresnet26',\n",
       " 'nf_seresnet50',\n",
       " 'nf_seresnet101',\n",
       " 'nfnet_f0',\n",
       " 'nfnet_f0s',\n",
       " 'nfnet_f1',\n",
       " 'nfnet_f1s',\n",
       " 'nfnet_f2',\n",
       " 'nfnet_f2s',\n",
       " 'nfnet_f3',\n",
       " 'nfnet_f3s',\n",
       " 'nfnet_f4',\n",
       " 'nfnet_f4s',\n",
       " 'nfnet_f5',\n",
       " 'nfnet_f5s',\n",
       " 'nfnet_f6',\n",
       " 'nfnet_f6s',\n",
       " 'nfnet_f7',\n",
       " 'nfnet_f7s',\n",
       " 'nfnet_l0a',\n",
       " 'nfnet_l0b',\n",
       " 'nfnet_l0c',\n",
       " 'pnasnet5large',\n",
       " 'regnetx_002',\n",
       " 'regnetx_004',\n",
       " 'regnetx_006',\n",
       " 'regnetx_008',\n",
       " 'regnetx_016',\n",
       " 'regnetx_032',\n",
       " 'regnetx_040',\n",
       " 'regnetx_064',\n",
       " 'regnetx_080',\n",
       " 'regnetx_120',\n",
       " 'regnetx_160',\n",
       " 'regnetx_320',\n",
       " 'regnety_002',\n",
       " 'regnety_004',\n",
       " 'regnety_006',\n",
       " 'regnety_008',\n",
       " 'regnety_016',\n",
       " 'regnety_032',\n",
       " 'regnety_040',\n",
       " 'regnety_064',\n",
       " 'regnety_080',\n",
       " 'regnety_120',\n",
       " 'regnety_160',\n",
       " 'regnety_320',\n",
       " 'repvgg_a2',\n",
       " 'repvgg_b0',\n",
       " 'repvgg_b1',\n",
       " 'repvgg_b1g4',\n",
       " 'repvgg_b2',\n",
       " 'repvgg_b2g4',\n",
       " 'repvgg_b3',\n",
       " 'repvgg_b3g4',\n",
       " 'res2net50_14w_8s',\n",
       " 'res2net50_26w_4s',\n",
       " 'res2net50_26w_6s',\n",
       " 'res2net50_26w_8s',\n",
       " 'res2net50_48w_2s',\n",
       " 'res2net101_26w_4s',\n",
       " 'res2next50',\n",
       " 'resnest14d',\n",
       " 'resnest26d',\n",
       " 'resnest50d',\n",
       " 'resnest50d_1s4x24d',\n",
       " 'resnest50d_4s2x40d',\n",
       " 'resnest101e',\n",
       " 'resnest200e',\n",
       " 'resnest269e',\n",
       " 'resnet18',\n",
       " 'resnet18d',\n",
       " 'resnet26',\n",
       " 'resnet26d',\n",
       " 'resnet34',\n",
       " 'resnet34d',\n",
       " 'resnet50',\n",
       " 'resnet50d',\n",
       " 'resnet101',\n",
       " 'resnet101d',\n",
       " 'resnet152',\n",
       " 'resnet152d',\n",
       " 'resnet200',\n",
       " 'resnet200d',\n",
       " 'resnetblur18',\n",
       " 'resnetblur50',\n",
       " 'resnetv2_50x1_bitm',\n",
       " 'resnetv2_50x1_bitm_in21k',\n",
       " 'resnetv2_50x3_bitm',\n",
       " 'resnetv2_50x3_bitm_in21k',\n",
       " 'resnetv2_101x1_bitm',\n",
       " 'resnetv2_101x1_bitm_in21k',\n",
       " 'resnetv2_101x3_bitm',\n",
       " 'resnetv2_101x3_bitm_in21k',\n",
       " 'resnetv2_152x2_bitm',\n",
       " 'resnetv2_152x2_bitm_in21k',\n",
       " 'resnetv2_152x4_bitm',\n",
       " 'resnetv2_152x4_bitm_in21k',\n",
       " 'resnext50_32x4d',\n",
       " 'resnext50d_32x4d',\n",
       " 'resnext101_32x4d',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'rexnet_100',\n",
       " 'rexnet_130',\n",
       " 'rexnet_150',\n",
       " 'rexnet_200',\n",
       " 'rexnetr_100',\n",
       " 'rexnetr_130',\n",
       " 'rexnetr_150',\n",
       " 'rexnetr_200',\n",
       " 'selecsls42',\n",
       " 'selecsls42b',\n",
       " 'selecsls60',\n",
       " 'selecsls60b',\n",
       " 'selecsls84',\n",
       " 'semnasnet_050',\n",
       " 'semnasnet_075',\n",
       " 'semnasnet_100',\n",
       " 'semnasnet_140',\n",
       " 'senet154',\n",
       " 'seresnet18',\n",
       " 'seresnet34',\n",
       " 'seresnet50',\n",
       " 'seresnet50t',\n",
       " 'seresnet101',\n",
       " 'seresnet152',\n",
       " 'seresnet152d',\n",
       " 'seresnet200d',\n",
       " 'seresnet269d',\n",
       " 'seresnext26d_32x4d',\n",
       " 'seresnext26t_32x4d',\n",
       " 'seresnext26tn_32x4d',\n",
       " 'seresnext50_32x4d',\n",
       " 'seresnext101_32x4d',\n",
       " 'seresnext101_32x8d',\n",
       " 'skresnet18',\n",
       " 'skresnet34',\n",
       " 'skresnet50',\n",
       " 'skresnet50d',\n",
       " 'skresnext50_32x4d',\n",
       " 'spnasnet_100',\n",
       " 'ssl_resnet18',\n",
       " 'ssl_resnet50',\n",
       " 'ssl_resnext50_32x4d',\n",
       " 'ssl_resnext101_32x4d',\n",
       " 'ssl_resnext101_32x8d',\n",
       " 'ssl_resnext101_32x16d',\n",
       " 'swsl_resnet18',\n",
       " 'swsl_resnet50',\n",
       " 'swsl_resnext50_32x4d',\n",
       " 'swsl_resnext101_32x4d',\n",
       " 'swsl_resnext101_32x8d',\n",
       " 'swsl_resnext101_32x16d',\n",
       " 'tf_efficientnet_b0',\n",
       " 'tf_efficientnet_b0_ap',\n",
       " 'tf_efficientnet_b0_ns',\n",
       " 'tf_efficientnet_b1',\n",
       " 'tf_efficientnet_b1_ap',\n",
       " 'tf_efficientnet_b1_ns',\n",
       " 'tf_efficientnet_b2',\n",
       " 'tf_efficientnet_b2_ap',\n",
       " 'tf_efficientnet_b2_ns',\n",
       " 'tf_efficientnet_b3',\n",
       " 'tf_efficientnet_b3_ap',\n",
       " 'tf_efficientnet_b3_ns',\n",
       " 'tf_efficientnet_b4',\n",
       " 'tf_efficientnet_b4_ap',\n",
       " 'tf_efficientnet_b4_ns',\n",
       " 'tf_efficientnet_b5',\n",
       " 'tf_efficientnet_b5_ap',\n",
       " 'tf_efficientnet_b5_ns',\n",
       " 'tf_efficientnet_b6',\n",
       " 'tf_efficientnet_b6_ap',\n",
       " 'tf_efficientnet_b6_ns',\n",
       " 'tf_efficientnet_b7',\n",
       " 'tf_efficientnet_b7_ap',\n",
       " 'tf_efficientnet_b7_ns',\n",
       " 'tf_efficientnet_b8',\n",
       " 'tf_efficientnet_b8_ap',\n",
       " 'tf_efficientnet_cc_b0_4e',\n",
       " 'tf_efficientnet_cc_b0_8e',\n",
       " 'tf_efficientnet_cc_b1_8e',\n",
       " 'tf_efficientnet_el',\n",
       " 'tf_efficientnet_em',\n",
       " 'tf_efficientnet_es',\n",
       " 'tf_efficientnet_l2_ns',\n",
       " 'tf_efficientnet_l2_ns_475',\n",
       " 'tf_efficientnet_lite0',\n",
       " 'tf_efficientnet_lite1',\n",
       " 'tf_efficientnet_lite2',\n",
       " 'tf_efficientnet_lite3',\n",
       " 'tf_efficientnet_lite4',\n",
       " 'tf_inception_v3',\n",
       " 'tf_mixnet_l',\n",
       " 'tf_mixnet_m',\n",
       " 'tf_mixnet_s',\n",
       " 'tf_mobilenetv3_large_075',\n",
       " 'tf_mobilenetv3_large_100',\n",
       " 'tf_mobilenetv3_large_minimal_100',\n",
       " 'tf_mobilenetv3_small_075',\n",
       " 'tf_mobilenetv3_small_100',\n",
       " 'tf_mobilenetv3_small_minimal_100',\n",
       " 'tresnet_l',\n",
       " 'tresnet_l_448',\n",
       " 'tresnet_m',\n",
       " 'tresnet_m_448',\n",
       " 'tresnet_xl',\n",
       " 'tresnet_xl_448',\n",
       " 'tv_densenet121',\n",
       " 'tv_resnet34',\n",
       " 'tv_resnet50',\n",
       " 'tv_resnet101',\n",
       " 'tv_resnet152',\n",
       " 'tv_resnext50_32x4d',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_base_patch16_224',\n",
       " 'vit_base_patch16_224_in21k',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_224',\n",
       " 'vit_base_patch32_224_in21k',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50_224_in21k',\n",
       " 'vit_base_resnet50_384',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_deit_base_distilled_patch16_224',\n",
       " 'vit_deit_base_distilled_patch16_384',\n",
       " 'vit_deit_base_patch16_224',\n",
       " 'vit_deit_base_patch16_384',\n",
       " 'vit_deit_small_distilled_patch16_224',\n",
       " 'vit_deit_small_patch16_224',\n",
       " 'vit_deit_tiny_distilled_patch16_224',\n",
       " 'vit_deit_tiny_patch16_224',\n",
       " 'vit_huge_patch14_224_in21k',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_224_in21k',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_224',\n",
       " 'vit_large_patch32_224_in21k',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s3_224',\n",
       " 'vovnet39a',\n",
       " 'vovnet57a',\n",
       " 'wide_resnet50_2',\n",
       " 'wide_resnet101_2',\n",
       " 'xception',\n",
       " 'xception41',\n",
       " 'xception65',\n",
       " 'xception71']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm \n",
    "\n",
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): SiLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pw): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "      (1): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "      (2): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pw): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
       "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "        (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
       "        (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=1056, bias=False)\n",
       "        (bn2): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
       "        (bn2): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
       "        (bn2): BatchNorm2d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): SiLU(inplace=True)\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n",
       "  (classifier): Linear(in_features=2048, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('efficientnet_b5', num_classes=11)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.classifier = nn.Sequential(\n",
    "     nn.Dropout(0.5),\n",
    "     nn.Linear(model.classifier.classifier, num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgd\n",
    "# adam\n",
    "# adamw\n",
    "# adamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.0001\n",
       "    momentum: 0.9\n",
       "    nesterov: True\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from timm.optim.optim_factory import create_optimizer\n",
    "from timm import create_model \n",
    "\n",
    "model = create_model('resnet34')\n",
    "\n",
    "args = SimpleNamespace()\n",
    "args.weight_decay = 0\n",
    "args.lr = 1e-4\n",
    "args.opt = 'sgd' #'lookahead_adam' to use `lookahead`\n",
    "args.momentum = 0.9\n",
    "\n",
    "optimizer = create_optimizer(args, model)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "scheduler = CosineLRScheduler(optimizer, t_initial=50, t_mul=1, lr_min=1e-5, decay_rate=1., cycle_limit=2, warmup_t=10, warmup_lr_init=1e-5)\n",
    "\n",
    "def get_lr_per_epoch(scheduler, num_epoch):\n",
    "    lr_per_epoch = []\n",
    "    for epoch in range(num_epoch):\n",
    "        lr_per_epoch.append(scheduler.get_epoch_values(epoch))\n",
    "    return lr_per_epoch\n",
    "lr_per_epoch = get_lr_per_epoch(scheduler, 50*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lr_per_epoch)\n",
    "plt.savefig('lr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhzdomRTOKoJ"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# You may choose where to download the data.\n",
    "\n",
    "# Google Drive\n",
    "# !gdown --id '1awF7pZ9Dz7X1jn1_QAiKN-_v56veCEKy' --output food-11.zip\n",
    "\n",
    "# Dropbox\n",
    "# !wget https://www.dropbox.com/s/m9q6273jl3djall/food-11.zip -O food-11.zip\n",
    "\n",
    "# MEGA\n",
    "# !sudo apt install megatools\n",
    "# !megadl \"https://mega.nz/#!zt1TTIhK!ZuMbg5ZjGWzWX1I6nEUbfjMZgCmAgeqJlwDkqdIryfg\"\n",
    "\n",
    "# Unzip the dataset.\n",
    "# This may take some time.\n",
    "# !unzip -q food-11.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBVSCWWhp6uq"
   },
   "source": [
    "## **Import Packages**\n",
    "\n",
    "First, we need to import packages that will be used later.\n",
    "\n",
    "In this homework, we highly rely on **torchvision**, a library of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9sVrKci4PUFW"
   },
   "outputs": [],
   "source": [
    "# Import necessary packages.\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from torchvision.datasets import DatasetFolder\n",
    "\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)\n",
    "print(torch.cuda.current_device())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0i9ZCPrOVN_"
   },
   "source": [
    "## **Dataset, Data Loader, and Transforms**\n",
    "\n",
    "Torchvision provides lots of useful utilities for image preprocessing, data wrapping as well as data augmentation.\n",
    "\n",
    "Here, since our data are stored in folders by class labels, we can directly apply **torchvision.datasets.DatasetFolder** for wrapping data without much effort.\n",
    "\n",
    "Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.auto_augment import rand_augment_transform\n",
    "from PIL import Image\n",
    "\n",
    "tfm = rand_augment_transform(\n",
    "    config_str='rand-m9-mstd0.5', \n",
    "    hparams={'translate_const': 117, 'img_mean': (124, 116, 104)}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RandAugment import RandAugment\n",
    "\n",
    "train_tfm = transforms.Compose([\n",
    "    tfm,\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# We don't need augmentations in testing and validation.\n",
    "# All we need here is to resize the PIL image and transform it into Tensor.\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "gKd2abixQghI"
   },
   "outputs": [],
   "source": [
    "# # It is important to do data augmentation in training.\n",
    "# # However, not every augmentation is useful.\n",
    "# # Please think about what kind of augmentation is helpful for food recognition.\n",
    "# train_tfm = transforms.Compose([\n",
    "#     # Resize the image into a fixed shape (height = width = 128)\n",
    "#     transforms.Resize((128, 128)),\n",
    "#     # You may add some transforms here.\n",
    "#     # ToTensor() should be the last one of the transforms.\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# # We don't need augmentations in testing and validation.\n",
    "# # All we need here is to resize the PIL image and transform it into Tensor.\n",
    "# test_tfm = transforms.Compose([\n",
    "#     transforms.Resize((128, 128)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qz6jeMnkQl0_"
   },
   "outputs": [],
   "source": [
    "# Batch size for training, validation, and testing.\n",
    "# A greater batch size usually gives a more stable gradient.\n",
    "# But the GPU memory is limited, so please adjust it carefully.\n",
    "batch_size = 128\n",
    "\n",
    "# Construct datasets.\n",
    "# The argument \"loader\" tells how torchvision reads the data.\n",
    "train_set = DatasetFolder(\"food-11/training/labeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "valid_set = DatasetFolder(\"food-11/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "\n",
    "unlabeled_set_clean = DatasetFolder(\"food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "unlabeled_set = DatasetFolder(\"food-11/training/unlabeled\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "test_set = DatasetFolder(\"food-11/testing\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "\n",
    "# Construct data loaders.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.samples\n",
    "good_indices = [1,2,3]\n",
    "train_set.samples = [train_set.samples[i] for i in good_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food-11/training/labeled/00/0_1.jpg', 0),\n",
       " ('food-11/training/labeled/00/0_10.jpg', 0),\n",
       " ('food-11/training/labeled/00/0_100.jpg', 0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'food-11/training/labeled/00/0_1.jpg'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.samples[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_set.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9YhZo7POPYG"
   },
   "source": [
    "## **Model**\n",
    "\n",
    "The basic model here is simply a stack of convolutional layers followed by some fully-connected layers.\n",
    "\n",
    "Since there are three channels for a color image (RGB), the input channels of the network must be three.\n",
    "In each convolutional layer, typically the channels of inputs grow, while the height and width shrink (or remain unchanged, according to some hyperparameters like stride and padding).\n",
    "\n",
    "Before fed into fully-connected layers, the feature map must be flattened into a single one-dimensional vector (for each image).\n",
    "These features are then transformed by the fully-connected layers, and finally, we obtain the \"logits\" for each class.\n",
    "\n",
    "### **WARNING -- You Must Know**\n",
    "You are free to modify the model architecture here for further improvement.\n",
    "However, if you want to use some well-known architectures such as ResNet50, please make sure **NOT** to load the pre-trained weights.\n",
    "Using such pre-trained models is considered cheating and therefore you will be punished.\n",
    "Similarly, it is your responsibility to make sure no pre-trained weights are used if you use **torch.hub** to load any modules.\n",
    "\n",
    "For example, if you use ResNet-18 as your model:\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=**False**)  This is fine.\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=**True**)   This is **NOT** allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1c-GwrMQqMl"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "        # input image size: [3, 128, 128]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input (x): [batch_size, 3, 128, 128]\n",
    "        # output: [batch_size, 11]\n",
    "\n",
    "        # Extract features by convolutional layers.\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "model = timm.create_model('tf_efficientnet_b5', pretrained=False, num_classes=11).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# from pprint import pprint\n",
    "# model_names = timm.list_models(pretrained=True)\n",
    "# pprint(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEnGbriXORN3"
   },
   "source": [
    "## **Training**\n",
    "\n",
    "You can finish supervised learning by simply running the provided code without any modification.\n",
    "\n",
    "The function \"get_pseudo_labels\" is used for semi-supervised learning.\n",
    "It is expected to get better performance if you use unlabeled data for semi-supervised learning.\n",
    "However, you have to implement the function on your own and need to adjust several hyperparameters manually.\n",
    "\n",
    "For more details about semi-supervised learning, please refer to [Prof. Lee's slides](https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/semi%20(v3).pdf).\n",
    "\n",
    "Again, please notice that utilizing external data (or pre-trained model) for training is **prohibited**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## same number sample each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swlf5EwA-hxA"
   },
   "outputs": [],
   "source": [
    "def get_pseudo_labels(dataset, model, threshold=0.65):\n",
    "    # This functions generates pseudo-labels of a dataset using given model.\n",
    "    # It returns an instance of DatasetFolder containing images whose prediction confidences exceed a given threshold.\n",
    "    # You are NOT allowed to use any models trained on external data for pseudo-labeling.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Make sure the model is in eval mode.\n",
    "    model.eval()\n",
    "    # Define softmax function.\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    # Iterate over the dataset by batches.\n",
    "    for batch in tqdm(dataloader):\n",
    "        img, _ = batch\n",
    "\n",
    "        # Forward the data\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = model(img.to(device))\n",
    "\n",
    "        # Obtain the probability distributions by applying softmax on logits.\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        # ---------- TODO ----------\n",
    "        # Filter the data and construct a new dataset.\n",
    "\n",
    "    # # Turn off the eval mode.\n",
    "    model.train()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHaFE-8oQtkC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# \"cuda\" only when GPUs are available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "# model = Classifier().to(device)\n",
    "model.device = device\n",
    "\n",
    "# For the classification task, we use cross-entropy as the measurement of performance.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "\n",
    "# The number of training epochs.\n",
    "n_epochs = 80\n",
    "\n",
    "# Whether to do semi-supervised learning.\n",
    "do_semi = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # ---------- TODO ----------\n",
    "    # In each epoch, relabel the unlabeled dataset for semi-supervised learning.\n",
    "    # Then you can combine the labeled dataset and pseudo-labeled dataset for the training.\n",
    "    if do_semi:\n",
    "        # Obtain pseudo-labels for unlabeled data using trained model.\n",
    "        pseudo_set = get_pseudo_labels(unlabeled_set, model)\n",
    "\n",
    "        # Construct a new dataset and a data loader for training.\n",
    "        # This is used in semi-supervised learning only.\n",
    "        concat_dataset = ConcatDataset([train_set, pseudo_set])\n",
    "        train_loader = DataLoader(concat_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    # Iterate the training set by batches.\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy of the training set is the average of the recorded values.\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "    model.eval()\n",
    "\n",
    "    # These are used to record information in validation.\n",
    "    valid_loss = []\n",
    "    valid_accs = []\n",
    "\n",
    "    # Iterate the validation set by batches.\n",
    "    for batch in tqdm(valid_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # We don't need gradient in validation.\n",
    "        # Using torch.no_grad() accelerates the forward process.\n",
    "        with torch.no_grad():\n",
    "            logits = model(imgs.to(device))\n",
    "\n",
    "        # We can still compute the loss (but not the gradient).\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        valid_loss.append(loss.item())\n",
    "        valid_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o1oCMXy61_3"
   },
   "source": [
    "## **Testing**\n",
    "\n",
    "For inference, we need to make sure the model is in eval mode, and the order of the dataset should not be shuffled (\"shuffle=False\" in test_loader).\n",
    "\n",
    "Last but not least, don't forget to save the predictions into a single CSV file.\n",
    "The format of CSV file should follow the rules mentioned in the slides.\n",
    "\n",
    "### **WARNING -- Keep in Mind**\n",
    "\n",
    "Cheating includes but not limited to:\n",
    "1.   using testing labels,\n",
    "2.   submitting results to previous Kaggle competitions,\n",
    "3.   sharing predictions with others,\n",
    "4.   copying codes from any creatures on Earth,\n",
    "5.   asking other people to do it for you.\n",
    "\n",
    "Any violations bring you punishments from getting a discount on the final grade to failing the course.\n",
    "\n",
    "It is your responsibility to check whether your code violates the rules.\n",
    "When citing codes from the Internet, you should know what these codes exactly do.\n",
    "You will **NOT** be tolerated if you break the rule and claim you don't know what these codes do.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HznI9_-ocrq"
   },
   "outputs": [],
   "source": [
    "# Make sure the model is in eval mode.\n",
    "# Some modules like Dropout or BatchNorm affect if the model is in training mode.\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store the predictions.Z\n",
    "predictions = []\n",
    "\n",
    "# Iterate the testing set by batches.\n",
    "for batch in tqdm(test_loader):\n",
    "    # A batch consists of image data and corresponding labels.\n",
    "    # But here the variable \"labels\" is useless since we do not have the ground-truth.\n",
    "    # If printing out the labels, you will find that it is always 0.\n",
    "    # This is because the wrapper (DatasetFolder) returns images and labels for each batch,\n",
    "    # so we have to create fake labels to make it work normally.\n",
    "    imgs, labels = batch\n",
    "\n",
    "    # We don't need gradient in testing, and we don't even have labels to compute loss.\n",
    "    # Using torch.no_grad() accelerates the forward process.\n",
    "    with torch.no_grad():\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "    # Take the class with greatest logit as prediction and record it.\n",
    "    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t2q2Th85ZUE"
   },
   "outputs": [],
   "source": [
    "# Save predictions into the file.\n",
    "with open(\"predict.csv\", \"w\") as f:\n",
    "\n",
    "    # The first row must be \"Id, Category\"\n",
    "    f.write(\"Id,Category\\n\")\n",
    "\n",
    "    # For the rest of the rows, each image id corresponds to a predicted class.\n",
    "    for i, pred in  enumerate(predictions):\n",
    "         f.write(f\"{i},{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
